{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут хранить через специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 4.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([[1,2,4],[1,2,4]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for 0_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 0_B\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(3, 3072, 10, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for 0_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 0_B\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(3, 3072, 10, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.232811, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.226305, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.238183, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.267008, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.141829, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.137928, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.191099, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.148836, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.036525, Train accuracy: 0.196889, val accuracy: 0.200000\n",
      "Loss: 2.113375, Train accuracy: 0.221889, val accuracy: 0.222000\n",
      "Loss: 2.091186, Train accuracy: 0.243667, val accuracy: 0.232000\n",
      "Loss: 2.087668, Train accuracy: 0.253333, val accuracy: 0.237000\n",
      "Loss: 1.948149, Train accuracy: 0.283222, val accuracy: 0.273000\n",
      "Loss: 1.832734, Train accuracy: 0.306111, val accuracy: 0.298000\n",
      "Loss: 1.806776, Train accuracy: 0.350889, val accuracy: 0.335000\n",
      "Loss: 2.011733, Train accuracy: 0.386000, val accuracy: 0.358000\n",
      "Loss: 1.716565, Train accuracy: 0.405000, val accuracy: 0.395000\n",
      "Loss: 1.851435, Train accuracy: 0.427889, val accuracy: 0.417000\n",
      "Loss: 1.756947, Train accuracy: 0.453889, val accuracy: 0.440000\n",
      "Loss: 1.649901, Train accuracy: 0.466000, val accuracy: 0.456000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(100, 3072, 10, reg = 0.001)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down and train and val accuracy go up for every epoch\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22100077470>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VNX5x/HPQ9gX2UEh7DuySljUqrigqAjuoiC4VIoVl9q61tr+sLXWVutSrEVFXBBUEEUE12pdkCXIvoc9gGyBsGZ/fn/MoENIyADJTJL5vl+vvDIz99yZJ5fJN4dzztxr7o6IiMSGMtEuQEREIkehLyISQxT6IiIxRKEvIhJDFPoiIjFEoS8iEkMU+iIiMUShLyISQxT6IiIxpGy0C8itTp063rRp02iXISJSosydO3eHu9ctqF2xC/2mTZuSmJgY7TJEREoUM1sfTjsN74iIxBCFvohIDFHoi4jEEIW+iEgMUeiLiMQQhb6ISAxR6IuIxBCFvohIlLk7Hy/ewoTZG4r8tYrdh7NERGLJnHUp/HXaMn7YsJuujWtwXfdGmFmRvZ5CX0QkCpK27ePJj5fz6dKt1D+pAn+7qiNXnRZfpIEPCn0RkYjatjeNZz5fxdtzNlKpXBz3XdSGW85sRqXycRF5fYW+iEgE7EvP4qWv1/DSN2vIyMrhxl5NuPO8ltSuWiGidSj0RUSKUGZ2DhPmbOTZz1eyY18Gl3Y6hfsubEPTOlWiUo9CX0SkCLg7nyzZypMfL2fNjv30aFaLl4a0pWvjmlGtS6EvIlLIEtel8Nfpy5m7fhct61Xl5SEJnN+u3tEnaVPWwr5t0Lhnkdam0BcRKSSrtwdW5HyyZCv1qlXgiSs7cnW3eMrGFfCRqAVvw0e/hZMawK9nQpmi+whVWKFvZn2BZ4E44GV3fyKfdlcD7wLd3T3RzJoCy4AVwSYz3X34iRYtIlLcPPP5Sp7/bxKVysXx2z6tufWsZlQuX0DEpu2Bab+DhW9Do15w1UtFGvgQRuibWRwwCugDJANzzGyKuy/N1a4acBcwK9dTrHb3LoVUr4hIsfPG9+t45vNVDOjSgD/0a0+dcFbkJCfCpFth9wbo/RCc9TuIK/rBl3D+pPQAktx9jbtnABOAAXm0ewx4EkgrxPpERIq1r1Zs408fLuX8tvV4+touBQd+TjZ88xSMuShw++bp0PvBiAQ+hBf6DYGNIfeTg4/9xMy6Ao3cfWoe+zczs3lm9j8zO+v4SxURKV6W/7iHEW/No039ajx3fVfiyhTwado9m+H1AfDFSGh3GQz/Fhr3ikyxQeH8acnrp/CfNpqVAf4J3JRHuy1AY3ffaWbdgPfN7FR333PYC5gNA4YBNG7cOMzSRUSiZ9veNG4dm0iVCnG8clMCVSoUEKfLP4IP7oCsdOj/L+g6GIr4lAt5Caennww0CrkfD2wOuV8N6AB8ZWbrgF7AFDNLcPd0d98J4O5zgdVA69wv4O6j3T3B3RPq1q17fD+JiEiEHMzI5rbXEknZn8ErQ7tzSvVK+TfOPAhT74UJN0CNxvCrr+G0G6MS+BBeT38O0MrMmgGbgIHADYc2unsqUOfQfTP7CvhdcPVOXSDF3bPNrDnQClhTiPWLiERUTo5z7zvzWbgplf8M7kaHhtXzb7x1CUy8BbYvh9NHwPmPQtnInnYhtwJD392zzGwE8AmBJZtj3H2JmY0EEt19ylF2PxsYaWZZQDYw3N1TCqNwEZFo+PunK5i++EceubQdF556ct6N3GH2S/DpI1CxOgx+D1qeH9lC8xHWdLG7TwOm5Xrs0Xza9g65PQmYdAL1iYgUG+/M2ci/v1rNoJ6NufUXzfJutH9HYOx+5cfQ6kIY8AJULT7D1vpErohIGGYk7eDhyYs4q1Ud/tT/1LxPqbD6S5g8HA6mQN8noOfwqI3d50ehLyJSgKRt+xj+5lya163CqEGnUS73aRXc4X9PwlePQ53WMHginNwxOsUWQKEvInIUO/elc8vYOZQvW4ZXhnbnpIrljmx0KPA7DYR+/4TylSNfaJgU+iIi+UjLzOZXb8xl6540JgzrRaNaeYT5d88GAr/zDTBgVJGfO+dEKfRFRPLg7jwwaSGJ63cx6obT8j4P/qzR8NmjcOqVMOBfxT7wIbwPZ4mIxJxnPl/FB/M3c99Fbbi00ylHNpj7Gky/D9pcCleOhjKRucbtiVLoi4jk8v68TTz7xSqu7hbPr3u3OLLBgrfhw7uh5QVwzasQl8c4fzGl0BcRCTF7bQr3T1xIr+a1ePyKjkcuzVzyPrw/HJr+Aq57M+qfsD1WCn0RkaB1O/bzqzcSia9ZiRcHd6N82VwRuWJ64Bz48T3g+glQ7ijn3CmmFPoiIkDqgUxuGTsHgDE3dadG5fKHN0j6At4ZElh/P+gdqFA1ClWeOK3eEZGYdyAji9teTyR510He/GVPmtapcniDdd/ChEFQp03gPDoVj3KStWJOPX0RiWlpmdnc9noiietTeOrazvRoVuvwBhtnw7hrA6dFHvI+VK6V9xOVEOrpi0jMSs/K5vY35zJj9U7+cXVnLuvc4PAGm+fDm1dDtfowdApUqZP3E5Ug6umLSEzKzM7hzrfm8eWK7fzl8o5c1S3+8AZbl8AblweGcoZMgWr5nEa5hFHoi0jMycrO4Z635/Pp0q386bL23NAz12Vat68MXMu2bMVAD79Go7yfqARS6ItITMnJce6fuJCPFm7h4UvactOZuc6Ln7IGXu8fuD1kCtTK57z5JZTG9EUkZuTkOA9PXsR78zbx2z6tGXZ2rk/b7t4Ir/WHrDS46SOoe8QlvUs89fRFJCa4O//34RImzNnIiHNbcuf5rQ5vsGcLvHYZpO2BG9+H+qdGp9Aipp6+iJR67s7j05bx2vfrue2sZvz2wlw9+L0/BoZ09m8PBH6DLtEpNAIU+iJS6j392Upe+mYtQ05vwsOXtDv8fDp7tsBr/QLfB0+ERt2jV2gEKPRFpFR7/otVPP/fJAZ2b8SfLst1bds9m2FsP9i3FQZPgianR6/QCFHoi0ipNfrr1Tz12Uqu7NqQv1zRkTJlQgI/dVOgh79ve+DUCo17Rq/QCFLoi0ip9NqMdTw+bTmXdjqFJ6/uRNxhgZ8c6OHv3wE3vgeNekSv0AhT6ItIqTN+9gb+OGUJfdrX55nrulA2LmSh4u6NgR7+gRS4cXKpH8PPTaEvIqXKpLnJPDx5Eb3b1OVfN3Sl3GGBvyHQwz+4O7BKJ75b9AqNEoW+iJQaHy7YzH0TF3BGi9q8OLgbFcqGXLd21/pADz8tNXC2zIanRa/QKFLoi0ip8PHiLdzz9nwSmtTipSEJVCwXGvjrYOxlkL4HhnwADbpGrc5oC+sTuWbW18xWmFmSmT14lHZXm5mbWULIYw8F91thZhcVRtEiIqHGfreWX4/7gU7x1Rlzc3cqlw/pz6asDQzpKPCBMHr6ZhYHjAL6AMnAHDOb4u5Lc7WrBtwFzAp5rD0wEDgVaAB8bmat3T278H4EEYlV2TnOY1OXMnbGOvq0r8+zA7vkCvw1gR5+5v7A2TJP6Ry9YouJcHr6PYAkd1/j7hnABGBAHu0eA54E0kIeGwBMcPd0d18LJAWfT0TkhOxPz2LY64mMnbGOX/6iGS8O7nZ44O9cHejhZx6AoR8q8IPCCf2GwMaQ+8nBx35iZl2BRu4+9Vj3FRE5Vj+mpnHtf77nyxXbeOzyDjzSr/3h6/APBX5WWiDwT+4YvWKLmXAmci2Px/ynjWZlgH8CNx3rviHPMQwYBtC4ceMjdhAROWTJ5lRuHZvI3rRMXrmpO+e2qXd4gx1JgVU62RmBwC+lZ8s8XuH09JOB0MvGxAObQ+5XAzoAX5nZOqAXMCU4mVvQvgC4+2h3T3D3hLp16x7bTyAiMeOLZVu55sXvKWMw8fYz8gj8VTD2UsjOhKFTFfh5CCf05wCtzKyZmZUnMDE75dBGd0919zru3tTdmwIzgf7unhhsN9DMKphZM6AVMLvQfwoRKfXGfreW215PpEXdqrx/x5m0O+WkwxtsXxkIfM+Gm6ZC/fbRKbSYK3B4x92zzGwE8AkQB4xx9yVmNhJIdPcpR9l3iZm9AywFsoA7tHJHRI5F6AqdC9vX55ncK3QAMtNg/HXgHujh12sbnWJLgLA+nOXu04BpuR57NJ+2vXPd/wvwl+OsT0Ri2P70LO4aP48vlm/jtrOa8eDF7Q6fsD1kxvOB5Zk3vq/AL4A+kSsixdKPqWncMnYOK7bu5c+Xd2BwryZ5N9y1Hr75B7S/HFqcG9kiSyCFvogUO4dW6OxLz2LMTd05p/VRFnh88jBYGbhIAwrhUOiLSLHyxbKt3Dl+HjUqlWPi7afT9uST8m+86jNYPhXO/yNUj49ckSWYQl9Eio1Xv1vLY1OX0qFhdV4ekkC9kyrm3zgzDabdB7VbwekjIldkCafQF5Fi4cmPl/PCV6u56NT6/PO6PFbo5Dbjedi1NnAhlLLlI1NkKaDQF5Gom702hRe+Ws11CY14/MqOea/QCbVrPXzzFLQfAC3Oi0yRpURYp1YWESkq6VnZPPTeQuJrVuKP/dsXHPgQnLw1uOjxoi+wlFHoi0hU/fur1azevp8/X96h4CEdgFWfByZvz75Pk7fHQaEvIlGTtG0fL3y5mgFdGtA793l08pKVDtPvg9otNXl7nDSmLyJRkZPjPPzeIiqVj+MP/cI8T86M5wKfvB38niZvj5N6+iISFe8kbmT2uhR+f0k76lStUPAOuzfA109Bu/7Q8vyiL7CUUuiLSMRt25vG49OW0at5La5JCHNc/uOHNHlbCBT6IhJxIz9cSlpmDn+5oiNmYazW+Wny9ndQo1HB7SVfCn0Riagvl29j6sItjDivJS3qVi14h0OTt7VaaPK2EGgiV0QiZn96Fo+8v5hW9aoy/JwW4e106LTJgydB2TDG/uWoFPoiEjH//Gwlm3Yf5N3hp1O+bBgDDbs3wNf/gHaXQcsLir7AGKDhHRGJiMWbUhnz3Vpu6NmY7k1rhbfTJw8Hvl/016IrLMYo9EWkyGVl5/DgewupXbUCD/QN88pWSZ/Dsg81eVvINLwjIkVu7Ix1LN60h1E3nEb1SuUK3iErHabdH5i8PePOoi8whij0RaRIbUw5wFOfruT8tvW4pOPJ4e0043lIWa3J2yKg4R0RKTLuzh8+WIwZjLy8Q3hr8ndvDEzetu2nydsioNAXkSIzdeEWvlqxnd9e2IaGNSqFt9Ohydu+mrwtCgp9ESkSqQcy+b8Pl9Apvjo3ndE0vJ2SvoBlU+Ds30KNxkVaX6zSmL6IFIknPl7GrgOZjL25R3gXRslKh+n3Q63mcMZdRV9gjFLoi0ihm7VmJ+Nnb+RXZzenQ8Pq4e004znYmQSDNHlblDS8IyKFKj0rm4cmLyK+ZiXuvqBVeDstmghfPh645m0rTd4WJfX0RaRQvfDlatZs38/Ym7uHd/nDZVPhvWHQqBdc/u+iLzDGhdXTN7O+ZrbCzJLM7ME8tg83s0VmNt/MvjWz9sHHm5rZweDj883sxcL+AUSk+Ejatpd/f3UMlz9c9Rm8exM06AqD3oHyVYq8xlhX4J9hM4sDRgF9gGRgjplNcfelIc3ecvcXg+37A08DfYPbVrt7l8ItW0SKm8DlDxeHf/nDNf+DtwdDvXaBD2FVqFb0RUpYPf0eQJK7r3H3DGACMCC0gbvvCblbBfDCK1FESoK3j+Xyh+u/h/EDAyt1bnwfKtWITJESVug3BDaG3E8OPnYYM7vDzFYDTwKh662amdk8M/ufmZ11QtWKSLF0ICOLf3yygh7Nwrj8YfJcGHcNnNQAhnwAVWpHpkgBwgv9vBbYHtGTd/dR7t4CeAB4JPjwFqCxu3cF7gXeMrOTjngBs2Fmlmhmidu3bw+/ehEpFt6cuZ6d+zN4oG/bo59qYctCePMKqFwLhkyBqmGM+0uhCif0k4HQ85rGA5uP0n4CcDmAu6e7+87g7bnAaqB17h3cfbS7J7h7Qt26dcOtXUSKgYMZ2Yz+eg1ntapDtyY182+4bTm8cTmUrwZDP4TqRwwYSASEE/pzgFZm1szMygMDgSmhDcwsdDHupcCq4ON1gxPBmFlzoBWwpjAKF5HiYdys9ezYl8Hd5x9lTf7O1fB6fyhTFoZOgZpNIlegHKbA1TvunmVmI4BPgDhgjLsvMbORQKK7TwFGmNkFQCawCxga3P1sYKSZZQHZwHB3TymKH0REIi8tM5v/fL2GM1vWJiG/q2HtWg+v9YecLLhpGtQO89q4UiTC+nCWu08DpuV67NGQ23fns98kYNKJFCgixddbszawfW86/7q+a94NUjfBa5dBxl4YOhXqhXnVLCkyOg2DiByXtMxsXvzfano1r0XP5nmswNm3LTCkcyAFbpwMp3SKfJFyBIW+iByXCbM3sG1vOneff8TaDNi/E14fAHs2w6B3oWG3yBcoedK5d0TkmKVlZvPv/62mR7NanN4iVy//4O7AKp2UNXDDO9Dk9OgUKXlST19Ejtm7iRvZuif9yBU76Xth3NWwbRlcNw6anxOdAiVf6umLyDFJz8rmha9Wk9CkJmeE9vIzDsBb18GmH+Da13WK5GJKPX0ROSbvJiazJTWNuy9o9fOnb91h0q2w4Xu46iVo1y+6RUq+1NMXkbBlZOXw769Wc1rjGvyiZZ2fNyz7EFZMgwv/DB2uil6BUiD19EUkbJN+SGbT7oPcfUHrn3v5Gfvh44egfgfoeXt0C5QCqacvImHJzM5h1JdJdG5Ug7NbhfTyv3kK9iTDVS9DnCKluFNPX0TC8t4PySTvOsg954eM5e9Igu+eg87Xa2lmCaHQF5ECZWbn8K8vk+gUX53ebYJnwnWH6fdDuUrQZ2R0C5SwKfRFpECT521iY8pB7g7t5S+fCqu/gHMf1nnxSxCFvogcVVZwLL9Dw5M4r20w3DMOBCZv650K3W+LboFyTBT6InJUH8zfzPqdB7jrvJBe/jdPQepGuPQfmrwtYRT6IpKvrOBYfvtTTqJP+/qBB3euhhnPQaeB0OSM6BYox0yhLyL5+nDhZtbu2M9dh8by3WHafVC2oiZvSyiFvojkKTvHef6/SbQ9uRoXHurlh07eVqsf3QLluCj0RSRPUxduZs32QC+/TBnT5G0poRkYETnCoV5+m/rV6HvqyYEHD03e3jxdk7clmHr6InKEaYu2kLRtH3ee3zLQy/9p8vY6Td6WcAp9ETlMTo7z/H9X0apeVS7pcErwk7cPBCdvH4t2eXKCFPoicpjpi39k5dZ9jDgv2Mtf/hEkfQa9H9LkbSmg0BeRnxzq5beoW4V+nRocPnnbY1i0y5NCoNAXkZ98uvRHlv+4lzvPa0VcGYNvn4bUDXDJ3zV5W0oo9EUECPTyn/0iieZ1qnBZ5waBydvvnoWO10LTM6NdnhQShb6IAPDZsq0s27KHEee1JM4ITN7GVYALNXlbmij0RYSU/Rn8bfpymtauTP/ODQLXu036DM59CKqdHO3ypBCFFfpm1tfMVphZkpk9mMf24Wa2yMzmm9m3ZtY+ZNtDwf1WmNlFhVm8iJy41AOZDH55Fpt2H+SJqzpRNjsNpj8I9dpr8rYUKjD0zSwOGAVcDLQHrg8N9aC33L2ju3cBngSeDu7bHhgInAr0BV4IPp+IFAN70zIZMmYWSdv2MXpIAr2a14Zv/xmcvP0HxJWLdolSyMLp6fcAktx9jbtnABOAAaEN3H1PyN0qgAdvDwAmuHu6u68FkoLPJyJRtj89i5tfncOSzXt4YdBpnNO6riZvY0A4a7AaAhtD7icDPXM3MrM7gHuB8sB5IfvOzLVvw+OqVEQKzcGMbG59bQ7zNu7mX9d35YL29QOfvP34QYgrr8nbUiycnr7l8Zgf8YD7KHdvATwAPHIs+5rZMDNLNLPE7du3h1GSiByvtMxshr2RyKy1KTx9bWcu7nhKYMOK6bDqU03elnLhhH4y0Cjkfjyw+SjtJwCXH8u+7j7a3RPcPaFu3bphlCQixyMjK4cRb/3AN6t28LerOjGgS/A/3rs3wtTfQN12mrwt5cIJ/TlAKzNrZmblCUzMTgltYGatQu5eCqwK3p4CDDSzCmbWDGgFzD7xskXkWGVl53D3hHl8vmwbf768A9cmBPtjB3fBuKsh8wBc/Yomb0u5Asf03T3LzEYAnwBxwBh3X2JmI4FEd58CjDCzC4BMYBcwNLjvEjN7B1gKZAF3uHt2Ef0sIpKP7Bznt+8uYPriH/lDv/YM7tUksCEzDSYMgpQ1MHgS1D81uoVKkTP3I4bYoyohIcETExOjXYZIqZGT4zwwaSHvzk3mgb5tub13i0MbYNItsGQyXPUKdLw6uoXKCTGzue6eUFA7fSJXpBRzd/7wwWLenZvMPRe0+jnwAT59JBD4fR5T4McQhb5IKeXujJy6lHGzNnB77xbcfX7I1Nv3o2DmKOg5HM64M3pFSsQp9EVKIXfnbx+v4NXv1nHLmc24/6I2mAVXUC+eBJ88DO0HwEWPg+W1slpKK4W+SCn07BerePF/qxncqzF/6Nfu58Bf+w1MHg6Nz4ArRkMZnRUl1ij0RUqZF75K4pnPV3FNt3hG9u/wc+BvXRpYqVOzGQwcB+UqRrdQiQqFvkgp8sq3a3ny4xUM6NKAJ67qFLjGLUDqpsBa/HKVAkszK9eKbqESNbr+mUgpMW7Weh6bupSLO5zMU9d0DlzuECAtFcZdA2l74JbpUKPR0Z9ISjWFvkgpkLguhUc/WMJ5bevx7MCulI0L/ic+Kz0wpLNjBQyaCCd3jG6hEnUKfZESbtf+DO4aP4+GNSrxzMAulC8bDPycHHj/17Dum8CkbYtzo1uoFAsKfZESzN25b+ICtu9LZ9LtZ3BSxZDz5nz+R1g8Ec7/I3S+LnpFSrGiiVyREmzMd+v4fNk2Hrq4HZ3ia/y8YeaLMOM56P5L+MVvolegFDsKfZESasHG3TwxfRkXtKvPzWc2/XnD0g8CF0Np2w8uflIfvpLDKPRFSqA9aZmMGP8DdatW4B/XdPp5Lf76GTDpNojvDle9rA9fyRE0pi9Swrg7D05ayObdabzzq17UqFw+sGH7Chh/fWBJ5g1vB9bki+Sinr5ICTNu1gamLfqR313Yhm5Ngh+yyjwI4wcGrm+rD1/JUainL1KCLN28h5FTl3J267r86uzmP2/45qnAhVCGfAA1m0atPin+1NMXKSH2p2cx4q0fqFGpHE9f2/nnUyxsXwnfPgOdroPmvaNZopQA6umLlADuziPvL2bdzv2M+2Uv6lStcGgDfHQvlK8MF/45ukVKiaDQFykBJs5NZvK8TdxzQStOb1H75w0L3wl84vbSp6FqvegVKCWGhndEirlVW/fy6AdL6NW8FneeF3L1q4O74NPfQ8ME6HZz9AqUEkU9fZFi7GBGNiPemkfl8nE8O7Drz2fOBPhiJBzYGVitU0b9NwmP3ikixdjIqUtYsXUvT1/XhfonhVz0JDkREl8NXOP2lM7RK1BKHIW+SDH1wfxNjJ+9kdt7t+Cc1nV/3pCdBVPvgWqnwLkPR69AKZE0vCNSDK3dsZ+H31tEtyY1ubdP68M3zh4NPy6Ca1+HCtWiU6CUWOrpixQz6VnZjHjrB8rGleG567tSLi7k1zR1E3z5F2jZB9r1j16RUmKppy9SzPx12nKWbN7DS0MSaFgj1/lzPnkIcrLgkr/r7JlyXNTTFylGPl78I2NnrOOWM5vRp339wzeu/DRw2uSz74NazaJToJR4Cn2RYmJjygHun7iATvHVefDitodvzDgA034HddrAGXdFp0ApFcIKfTPra2YrzCzJzB7MY/u9ZrbUzBaa2Rdm1iRkW7aZzQ9+TSnM4kVKg/SsbCbNTWbomNm4w/PXd/35OreHfPMU7F4Plz4FZctHp1ApFQoc0zezOGAU0AdIBuaY2RR3XxrSbB6Q4O4HzOx24Eng0EU5D7p7l0KuW6TE2743nXGz1vPmzA3s2JdOy3pV+ffgbjSpXSVXwxXw3bPQ+XpodlZ0ipVSI5yJ3B5AkruvATCzCcAA4KfQd/cvQ9rPBAYXZpEipcniTam8+t06PlywmYzsHM5tU5ebz2zGWa3q/HwFrEPcYeq9UL4K9HksOgVLqRJO6DcENobcTwZ6HqX9rcD0kPsVzSwRyAKecPf3c+9gZsOAYQCNGzcOoySRkiU7x/ls6VbGfLeW2WtTqFQujoE9GjH0jKa0qFs1/x0XTID138Jlz0LVuvm3EwlTOKGf17owz7Oh2WAgATgn5OHG7r7ZzJoD/zWzRe6++rAncx8NjAZISEjI87lFSqI9aZm8M2cjY2esI3nXQRrWqMTDl7TluoTGVK9c7ug7H0iBTx+B+B7QdUhkCpZSL5zQTwYahdyPBzbnbmRmFwC/B85x9/RDj7v75uD3NWb2FdAVWJ17f5HSZO2O/Yz9bi0T5yazPyObHk1r8ftL2tGnfX3KxoW5aO6L/wucSbPf0zqhmhSacEJ/DtDKzJoBm4CBwA2hDcysK/AfoK+7bwt5vCZwwN3TzawOcCaBSV6RUsfd+S5pJ69+t5b/rthG2TLGZZ0bcMuZzejQsPqxPdnG2TB3LJw+Ak7uWCT1SmwqMPTdPcvMRgCfAHHAGHdfYmYjgUR3nwL8HagKvBuciNrg7v2BdsB/zCyHwPLQJ3Kt+hEpFXYfyODW1xKZu34XdaqW567zWjGoV2PqVatY8M65ZWfB1N/ASQ2h90OFX6zEtLBOw+Du04BpuR57NOT2BfnsNwNQN0VKtZT9GQx6eRart+/j8Ss6cuVpDalYLu74n3DWi7B1MVz3JlQ4yiSvyHHQuXdETsCOfekMfnkWa3fs5+UhCZzd+gRX2KQmw5ePQ+u+0LZf4RQpEkKhL3Kctu1NY9BLs9i46wCv3tSdM1rWOfEn/fhB8By4+EmdUE2KhJYEiByHrXvSGDh6Jpt2H2TszT0KJ/BXfgLLPoRz7oeaTQpuL3Ic1NMXOUZbUg9y/eiZbN+bzmu39KB701rH/iTusGsdJM+BjbMCq3W2Loa6bQMrdkSKiEJf5Bgk7zrADS/NYtf+DF6/tSfdmtQMb8fMNNgyPxDuh0J+f3B1c/m9c8E7AAALaElEQVRqEN8tcMrkroN1QjUpUgp9kTBtTDnAwNEz2ZuWyRu/7EmXRjXyb7xnczDgZ0PybNg8H3IyA9tqNYcW50GjHtCoJ9RrB2VOYLWPyDFQ6IuEYf3O/Vw/eib7M7J567ZeR37YKvMgLJoIa74MBH1q8HRVZStCg65w+q8DAR/fQ+fQkahS6IsUYM32fdzw0izSs7J567aenNogJPD374A5L8Psl+DADqjWINCD7xUM+ZM7arhGihWFvshRJG3bxw0vzSQ7xxk/rBdtTz4psGHHKvh+FCwYD1lpgXX1Z9wJTc7UUksp1hT6IvlYuXUvN7w0C4AJw3rRql5VWPcdfP8vWDEN4ipA54Fw+h1Qt02UqxUJj0JfJA/Ltuxh8MuziCtjvHVrAi23fwofPA+b50GlWnDOA9D9No3PS4mj0BfJZcnmVAa/PIuacRm823MVtcf/BlI3QK0WcOnTgcsWlq8c7TJFjotCXyTEouRU7n15Gr+L+5jr7b+U+XYPND4DLn4CWl+s89pLiafQFwlavmAW6yY/xnRmEJfjWNsBcPqdgQ9OiZQSCn0RYOOcD2n20c00Jo60LrdQ9ZwRULNptMsSKXQKfYl5KQs+ot5HN7OehlS97UMaNGwc7ZJEiowGKCWmHVgyjaqTh7Da4/EhUxT4Uuop9CVmZS6bTrl3h7AypxF7rplIm+Y6nbGUfgp9iUk5y6Zhbw9mWU486y4dR68OLaNdkkhEKPQl9iyfhr9zI4tzGjP7rFfp1/PUaFckEjEKfYkty6aS/faNLMpuwkedXuDWC7pEuyKRiNLqHYkdyz4k552bWJjdlLHNn+LpK3thOjmaxBiFvsSGpR+Q8+4tzM9pxlP1/sorg84irowCX2KPQl9KvyWT8Ym3ssBb8mi1P/HGzedQsZyuVCWxSaEvpdviSfik21hIa+6O+z3jbulNzSq6qInELoW+lF6LJuLv3cbiuHb8MuN+Xv3l2TSqpbNjSmxT6EvptPBdfPIwlpfrwKB9v+H5m8488rq2IjEorCWbZtbXzFaYWZKZPZjH9nvNbKmZLTSzL8ysSci2oWa2Kvg1tDCLF8nTgrfxycNYXakTV+65hz9c2Z1zWutiJyIQRuibWRwwCrgYaA9cb2btczWbByS4eydgIvBkcN9awB+BnkAP4I9mVrPwyhfJZf54mPwrNlQ7jX4pd/PrPp24JqFRtKsSKTbC6en3AJLcfY27ZwATgAGhDdz9S3c/ELw7E4gP3r4I+MzdU9x9F/AZ0LdwShcJsfdH+P4FeP92ttTuyUXb7uCKHq0YcZ5OryASKpwx/YbAxpD7yQR67vm5FZh+lH0b5t7BzIYBwwAaN9ZZDqUA2VmwdTEkz4GNswJfuzcAsKP+Lzh3wy/5Rbt4Hhtwqj58JZJLOKGf12+N59nQbDCQAJxzLPu6+2hgNEBCQkKezy0x7EBKMOBnBwJ+0w+QuT+wrdop0KgH9BzOsri2XDkljTbxNXju+q6UjdNZRkRyCyf0k4HQQdF4YHPuRmZ2AfB74Bx3Tw/Zt3eufb86nkIlRuTkwI4VwYCfDcmzYcfKwDaLg5M7QtfBHDi5G8vi2jJnVxUWbkpl4depJO86SNPalRkzNIHK5bUwTSQv4fxmzAFamVkzYBMwELghtIGZdQX+A/R1920hmz4BHg+ZvL0QeOiEq87Lwd0wfmCRPLVESE52IPDTUgP3K9WERj3J7HAdayq1Z1ZaU37Yks7CZams+Xo/gT4FNKpVic6NanBjryZc0bUhtatWiN7PIFLMFRj67p5lZiMIBHgcMMbdl5jZSCDR3acAfweqAu8Gx1A3uHt/d08xs8cI/OEAGOnuKUXykwDElSuyp5YIiCtHdrvL2VytEz94K2ak1GDBplRWLd5Hdo4Da6l/UgU6xdfgii4N6dSoBp0aVtcnbEWOgbkXryH0hIQET0xMPOb9dh/I4JoXvy+CiiRSst1JTjlIRnYOADUrl6NTfA06xVf/6Xv9kypGuUqR4snM5rp7QkHtSs3AZ5kyRqv6VaNdhpygPu3q/xTw8TUrafWNSCErNaF/UsVyvDCoW7TLEBEp1rSmTUQkhij0RURiiEJfRCSGKPRFRGKIQl9EJIYo9EVEYohCX0Qkhij0RURiSLE7DYOZbQfWn8BT1AF2FFI5RUH1nRjVd2JU34kpzvU1cfcCrwta7EL/RJlZYjjnn4gW1XdiVN+JUX0nprjXFw4N74iIxBCFvohIDCmNoT862gUUQPWdGNV3YlTfiSnu9RWo1I3pi4hI/kpjT19ERPJRIkPfzPqa2QozSzKzB/PYXsHM3g5un2VmTSNYWyMz+9LMlpnZEjO7O482vc0s1czmB78ejVR9ITWsM7NFwdc/4lJlFvBc8BguNLPTIlhbm5BjM9/M9pjZPbnaRPQYmtkYM9tmZotDHqtlZp+Z2arg95r57Ds02GaVmQ2NYH1/N7PlwX+/yWZWI599j/peKML6/mRmm0L+DS/JZ9+j/r4XYX1vh9S2zszm57NvkR+/QuXuJeqLwHV6VwPNgfLAAqB9rja/Bl4M3h4IvB3B+k4BTgvergaszKO+3sDUKB/HdUCdo2y/BJgOGNALmBXFf+8fCaxBjtoxBM4GTgMWhzz2JPBg8PaDwN/y2K8WsCb4vWbwds0I1XchUDZ4+2951RfOe6EI6/sT8Lsw/v2P+vteVPXl2v4U8Gi0jl9hfpXEnn4PIMnd17h7BjABGJCrzQDgteDticD5FqHr7rn7Fnf/IXh7L7AMaBiJ1y5kA4DXPWAmUMPMTolCHecDq939RD6wd8Lc/WsgJdfDoe+z14DL89j1IuAzd09x913AZ0DfSNTn7p+6e1bw7kwgvrBfN1z5HL9whPP7fsKOVl8wO64Fxhf260ZDSQz9hsDGkPvJHBmqP7UJvulTgdoRqS5EcFipKzArj82nm9kCM5tuZqdGtLAABz41s7lmNiyP7eEc50gYSP6/bNE+hvXdfQsE/tgD9fJoU1yO4y0E/ueWl4LeC0VpRHD4aUw+w2PF4fidBWx191X5bI/m8TtmJTH08+qx516CFE6bImVmVYFJwD3uvifX5h8IDFd0Bp4H3o9kbUFnuvtpwMXAHWZ2dq7txeEYlgf6A+/msbk4HMNwFIfj+HsgCxiXT5OC3gtF5d9AC6ALsIXAEEpuUT9+wPUcvZcfreN3XEpi6CcDjULuxwOb82tjZmWB6hzffy2Pi5mVIxD449z9vdzb3X2Pu+8L3p4GlDOzOpGqL/i6m4PftwGTCfw3OlQ4x7moXQz84O5bc28oDscQ2HpoyCv4fVsebaJ6HIMTx/2AQR4cgM4tjPdCkXD3re6e7e45wEv5vG60j19Z4Erg7fzaROv4Ha+SGPpzgFZm1izYExwITMnVZgpwaJXE1cB/83vDF7bg+N8rwDJ3fzqfNicfmmMwsx4E/h12RqK+4GtWMbNqh24TmPBbnKvZFGBIcBVPLyD10FBGBOXbw4r2MQwKfZ8NBT7Io80nwIVmVjM4fHFh8LEiZ2Z9gQeA/u5+IJ824bwXiqq+0DmiK/J53XB+34vSBcByd0/Oa2M0j99xi/ZM8vF8EVhZspLArP7vg4+NJPDmBqhIYEggCZgNNI9gbb8g8N/PhcD84NclwHBgeLDNCGAJgZUIM4EzInz8mgdfe0GwjkPHMLRGA0YFj/EiICHCNVYmEOLVQx6L2jEk8MdnC5BJoPd5K4F5oi+AVcHvtYJtE4CXQ/a9JfheTAJujmB9SQTGww+9Dw+taGsATDvaeyFC9b0RfG8tJBDkp+SuL3j/iN/3SNQXfHzsofdcSNuIH7/C/NInckVEYkhJHN4REZHjpNAXEYkhCn0RkRii0BcRiSEKfRGRGKLQFxGJIQp9EZEYotAXEYkh/w+w8RtLbLssZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        result = ... # промежуточные вычисления\n",
    "        self.x = x # сохраняем значения, которые нам\n",
    "                   # понадобятся при обратном проходе\n",
    "        return result\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        dx = ... # используем сохраненные значения, чтобы \n",
    "        dw = ... # вычислить градиент по x и по w\n",
    "        self.w.grad += dw # аккумулируем градиент dw\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.233943, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.227963, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.116775, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.260924, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.240014, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.357802, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.262062, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.166126, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.088272, Train accuracy: 0.192222, val accuracy: 0.195000\n",
      "Loss: 2.111706, Train accuracy: 0.212333, val accuracy: 0.213000\n",
      "Loss: 2.305725, Train accuracy: 0.243000, val accuracy: 0.238000\n",
      "Loss: 2.155386, Train accuracy: 0.259667, val accuracy: 0.253000\n",
      "Loss: 1.728261, Train accuracy: 0.280000, val accuracy: 0.273000\n",
      "Loss: 2.109735, Train accuracy: 0.304222, val accuracy: 0.293000\n",
      "Loss: 1.890930, Train accuracy: 0.322556, val accuracy: 0.311000\n",
      "Loss: 1.915306, Train accuracy: 0.341667, val accuracy: 0.319000\n",
      "Loss: 2.135395, Train accuracy: 0.360667, val accuracy: 0.343000\n",
      "Loss: 2.313308, Train accuracy: 0.385444, val accuracy: 0.372000\n",
      "Loss: 1.928113, Train accuracy: 0.396889, val accuracy: 0.383000\n",
      "Loss: 1.865428, Train accuracy: 0.429222, val accuracy: 0.423000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(100, 3072, 10, reg = 0.001)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99, learning_rate=1e-2)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.314890, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.247814, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.215954, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.185074, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.463703, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.050778, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 1.989586, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.002382, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.098380, Train accuracy: 0.192333, val accuracy: 0.195000\n",
      "Loss: 2.238386, Train accuracy: 0.212000, val accuracy: 0.221000\n",
      "Loss: 2.283021, Train accuracy: 0.241556, val accuracy: 0.237000\n",
      "Loss: 2.379846, Train accuracy: 0.260000, val accuracy: 0.252000\n",
      "Loss: 1.706586, Train accuracy: 0.284889, val accuracy: 0.282000\n",
      "Loss: 1.969569, Train accuracy: 0.306778, val accuracy: 0.295000\n",
      "Loss: 1.734875, Train accuracy: 0.320000, val accuracy: 0.312000\n",
      "Loss: 2.193988, Train accuracy: 0.338667, val accuracy: 0.328000\n",
      "Loss: 2.006003, Train accuracy: 0.363222, val accuracy: 0.347000\n",
      "Loss: 1.962038, Train accuracy: 0.390667, val accuracy: 0.384000\n",
      "Loss: 1.415255, Train accuracy: 0.422111, val accuracy: 0.409000\n",
      "Loss: 1.449799, Train accuracy: 0.434222, val accuracy: 0.423000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(100, 3072, 10, reg = 1e-3)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-3, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.322703, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.320555, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.286107, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.304149, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.264015, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.283399, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.227245, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.228731, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.219130, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.244057, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.143188, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.197151, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.246096, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.305099, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.070014, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.041618, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.974307, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.808203, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.685264, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.293673, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.785197, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.116086, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.760481, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.696229, Train accuracy: 0.600000, val accuracy: 0.133333\n",
      "Loss: 1.648209, Train accuracy: 0.600000, val accuracy: 0.133333\n",
      "Loss: 1.421080, Train accuracy: 0.600000, val accuracy: 0.133333\n",
      "Loss: 1.493337, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.413692, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.898338, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.784977, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.896775, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.254393, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.815912, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.046035, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.983946, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.605327, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.427181, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 2.030103, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.368498, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.989530, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.190372, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.446736, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.293763, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 0.971979, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.465864, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.621726, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.134121, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.190155, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.441358, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.042155, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.063342, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.794841, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.127940, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.103325, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.128900, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.254842, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.223623, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.257054, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.112985, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 1.137450, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.389711, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 0.953616, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.541123, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 0.888296, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.095082, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.531366, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.481660, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.187844, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.124368, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.074284, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.189584, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.251590, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.616463, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.672042, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.231975, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.420329, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.142946, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.289461, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.569719, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.242248, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.086098, Train accuracy: 0.933333, val accuracy: 0.133333\n",
      "Loss: 1.271102, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.468439, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.308823, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.731575, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.146105, Train accuracy: 0.933333, val accuracy: 0.133333\n",
      "Loss: 1.282563, Train accuracy: 0.933333, val accuracy: 0.133333\n",
      "Loss: 1.632439, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.242525, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.331014, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.127243, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.377374, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.323477, Train accuracy: 0.933333, val accuracy: 0.133333\n",
      "Loss: 1.080355, Train accuracy: 0.933333, val accuracy: 0.133333\n",
      "Loss: 1.380328, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.536680, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.246576, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.398211, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.335552, Train accuracy: 0.933333, val accuracy: 0.133333\n",
      "Loss: 1.266068, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.329992, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.240854, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.373001, Train accuracy: 0.933333, val accuracy: 0.133333\n",
      "Loss: 1.051799, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.599885, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.305278, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.082279, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 0.918744, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.172937, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.240689, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.346825, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.321860, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.409079, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.356338, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.212361, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.446231, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.344477, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.316621, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.079200, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.148015, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.605433, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.214994, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.273005, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.403325, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.134726, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.192109, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.067682, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.541406, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.161870, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.200930, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.203967, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.238102, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.100340, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.226426, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.139279, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.254805, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.321891, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.069557, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.150733, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.169298, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.156546, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.203339, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.337833, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.121633, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.284843, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.319638, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.296577, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.130986, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.134598, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 1.264735, Train accuracy: 1.000000, val accuracy: 0.133333\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(100, 3072, 10, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.316296, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.328327, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: 2.178138, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.149553, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.035125, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.100496, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.198599, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.979404, Train accuracy: 0.533333, val accuracy: 0.133333\n",
      "Loss: 1.787430, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.014876, Train accuracy: 0.533333, val accuracy: 0.200000\n",
      "Loss: 1.126851, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.153090, Train accuracy: 0.600000, val accuracy: 0.133333\n",
      "Loss: 0.668097, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 0.433572, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 0.582129, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 0.202058, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 0.791773, Train accuracy: 0.866667, val accuracy: 0.133333\n",
      "Loss: 1.183723, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 0.206569, Train accuracy: 1.000000, val accuracy: 0.133333\n",
      "Loss: 0.051763, Train accuracy: 1.000000, val accuracy: 0.133333\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(100, 3072, 10, reg = 1e-3)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=0.3, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **40%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
