{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip3 -qq install bokeh==0.13.0\n",
    "!pip3 -qq install gensim==3.6.0\n",
    "!pip3 -qq install nltk\n",
    "!pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'.', 'X', 'NUM', 'PRT', 'VERB', 'ADV', 'DET', 'ADP', 'NOUN', 'CONJ', 'ADJ', 'PRON'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 1,\n",
       " 'X': 2,\n",
       " 'NUM': 3,\n",
       " 'PRT': 4,\n",
       " 'VERB': 5,\n",
       " 'ADV': 6,\n",
       " 'DET': 7,\n",
       " 'ADP': 8,\n",
       " 'NOUN': 9,\n",
       " 'CONJ': 10,\n",
       " 'ADJ': 11,\n",
       " 'PRON': 12,\n",
       " '<pad>': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #print(\"Inputs: \", inputs.shape)\n",
    "        emb = self._emb(inputs)\n",
    "        #print(\"Emb: \", emb.shape)\n",
    "        output, _ = self._lstm(emb)\n",
    "        #print(\"Output of LSTM: \", output.shape)\n",
    "        out = self._out_layer(output)\n",
    "        #print(\"Final output\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  torch.Size([32, 4])\n",
      "Emb:  torch.Size([32, 4, 100])\n",
      "Output of LSTM:  torch.Size([32, 4, 128])\n",
      "Final output torch.Size([32, 4, 13])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16304347826086957"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "mask = (y_batch != 0).float()\n",
    "correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
    "total_count = mask.sum().item()\n",
    "\n",
    "correct_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5247, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion(logits.transpose(2, 1), y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                mask = (y_batch != 0).float()\n",
    "                cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
    "                cur_sum_count = mask.sum().item()\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.31122, Accuracy = 72.46%: 100%|██████████| 572/572 [00:06<00:00, 88.11it/s]\n",
      "[1 / 20]   Val: Loss = 0.10657, Accuracy = 84.83%: 100%|██████████| 13/13 [00:00<00:00, 102.36it/s]\n",
      "[2 / 20] Train: Loss = 0.09963, Accuracy = 90.03%: 100%|██████████| 572/572 [00:06<00:00, 92.27it/s]\n",
      "[2 / 20]   Val: Loss = 0.07874, Accuracy = 89.07%: 100%|██████████| 13/13 [00:00<00:00, 100.00it/s]\n",
      "[3 / 20] Train: Loss = 0.06764, Accuracy = 93.23%: 100%|██████████| 572/572 [00:06<00:00, 87.88it/s]\n",
      "[3 / 20]   Val: Loss = 0.07489, Accuracy = 90.95%: 100%|██████████| 13/13 [00:00<00:00, 101.56it/s]\n",
      "[4 / 20] Train: Loss = 0.05104, Accuracy = 94.83%: 100%|██████████| 572/572 [00:06<00:00, 86.60it/s]\n",
      "[4 / 20]   Val: Loss = 0.06344, Accuracy = 91.88%: 100%|██████████| 13/13 [00:00<00:00, 96.97it/s]\n",
      "[5 / 20] Train: Loss = 0.04061, Accuracy = 95.82%: 100%|██████████| 572/572 [00:06<00:00, 86.97it/s]\n",
      "[5 / 20]   Val: Loss = 0.06667, Accuracy = 92.41%: 100%|██████████| 13/13 [00:00<00:00, 100.00it/s]\n",
      "[6 / 20] Train: Loss = 0.03337, Accuracy = 96.59%: 100%|██████████| 572/572 [00:06<00:00, 87.27it/s]\n",
      "[6 / 20]   Val: Loss = 0.06356, Accuracy = 92.69%: 100%|██████████| 13/13 [00:00<00:00, 101.54it/s]\n",
      "[7 / 20] Train: Loss = 0.02736, Accuracy = 97.15%: 100%|██████████| 572/572 [00:06<00:00, 86.76it/s]\n",
      "[7 / 20]   Val: Loss = 0.06309, Accuracy = 92.93%: 100%|██████████| 13/13 [00:00<00:00, 100.78it/s]\n",
      "[8 / 20] Train: Loss = 0.02289, Accuracy = 97.63%: 100%|██████████| 572/572 [00:06<00:00, 87.29it/s]\n",
      "[8 / 20]   Val: Loss = 0.06606, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 98.49it/s]\n",
      "[9 / 20] Train: Loss = 0.01931, Accuracy = 98.00%: 100%|██████████| 572/572 [00:06<00:00, 86.83it/s]\n",
      "[9 / 20]   Val: Loss = 0.06559, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 98.44it/s]\n",
      "[10 / 20] Train: Loss = 0.01591, Accuracy = 98.34%: 100%|██████████| 572/572 [00:06<00:00, 86.37it/s]\n",
      "[10 / 20]   Val: Loss = 0.06823, Accuracy = 92.95%: 100%|██████████| 13/13 [00:00<00:00, 97.02it/s]\n",
      "[11 / 20] Train: Loss = 0.01327, Accuracy = 98.62%: 100%|██████████| 572/572 [00:06<00:00, 86.19it/s]\n",
      "[11 / 20]   Val: Loss = 0.07502, Accuracy = 92.99%: 100%|██████████| 13/13 [00:00<00:00, 98.11it/s]\n",
      "[12 / 20] Train: Loss = 0.01104, Accuracy = 98.88%: 100%|██████████| 572/572 [00:06<00:00, 86.63it/s]\n",
      "[12 / 20]   Val: Loss = 0.07652, Accuracy = 92.95%: 100%|██████████| 13/13 [00:00<00:00, 98.35it/s]\n",
      "[13 / 20] Train: Loss = 0.00897, Accuracy = 99.09%: 100%|██████████| 572/572 [00:06<00:00, 86.47it/s]\n",
      "[13 / 20]   Val: Loss = 0.07605, Accuracy = 92.90%: 100%|██████████| 13/13 [00:00<00:00, 96.25it/s]\n",
      "[14 / 20] Train: Loss = 0.00737, Accuracy = 99.28%: 100%|██████████| 572/572 [00:06<00:00, 85.39it/s] \n",
      "[14 / 20]   Val: Loss = 0.08220, Accuracy = 92.98%: 100%|██████████| 13/13 [00:00<00:00, 97.75it/s] \n",
      "[15 / 20] Train: Loss = 0.00601, Accuracy = 99.43%: 100%|██████████| 572/572 [00:06<00:00, 86.31it/s]\n",
      "[15 / 20]   Val: Loss = 0.08345, Accuracy = 92.88%: 100%|██████████| 13/13 [00:00<00:00, 97.72it/s]\n",
      "[16 / 20] Train: Loss = 0.00499, Accuracy = 99.54%: 100%|██████████| 572/572 [00:06<00:00, 87.15it/s]\n",
      "[16 / 20]   Val: Loss = 0.08393, Accuracy = 92.82%: 100%|██████████| 13/13 [00:00<00:00, 97.74it/s]\n",
      "[17 / 20] Train: Loss = 0.00407, Accuracy = 99.63%: 100%|██████████| 572/572 [00:06<00:00, 86.69it/s]\n",
      "[17 / 20]   Val: Loss = 0.09511, Accuracy = 92.84%: 100%|██████████| 13/13 [00:00<00:00, 100.75it/s]\n",
      "[18 / 20] Train: Loss = 0.00330, Accuracy = 99.70%: 100%|██████████| 572/572 [00:06<00:00, 86.41it/s]\n",
      "[18 / 20]   Val: Loss = 0.10170, Accuracy = 92.76%: 100%|██████████| 13/13 [00:00<00:00, 99.24it/s]\n",
      "[19 / 20] Train: Loss = 0.00281, Accuracy = 99.75%: 100%|██████████| 572/572 [00:06<00:00, 86.40it/s]\n",
      "[19 / 20]   Val: Loss = 0.09733, Accuracy = 92.70%: 100%|██████████| 13/13 [00:00<00:00, 97.02it/s]\n",
      "[20 / 20] Train: Loss = 0.00249, Accuracy = 99.77%: 100%|██████████| 572/572 [00:06<00:00, 86.57it/s]\n",
      "[20 / 20]   Val: Loss = 0.09798, Accuracy = 92.76%: 100%|██████████| 13/13 [00:00<00:00, 99.22it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 5.43%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.5756, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "mask = (y_batch != 0).float()\n",
    "correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
    "total_count = mask.sum().item()\n",
    "accuracy = (correct_count / total_count)\n",
    "\n",
    "print('Accuracy = {:.2%}'.format(accuracy))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "criterion(logits.transpose(2, 1), y_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self._out_layer(self._lstm(self._emb(inputs))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.55044, Accuracy = 82.58%: 100%|██████████| 572/572 [00:07<00:00, 74.06it/s]\n",
      "[1 / 20]   Val: Loss = 0.27354, Accuracy = 91.12%: 100%|██████████| 13/13 [00:00<00:00, 72.80it/s]\n",
      "[2 / 20] Train: Loss = 0.20464, Accuracy = 93.52%: 100%|██████████| 572/572 [00:07<00:00, 73.18it/s]\n",
      "[2 / 20]   Val: Loss = 0.17728, Accuracy = 94.31%: 100%|██████████| 13/13 [00:00<00:00, 77.27it/s]\n",
      "[3 / 20] Train: Loss = 0.12916, Accuracy = 96.02%: 100%|██████████| 572/572 [00:07<00:00, 73.34it/s]\n",
      "[3 / 20]   Val: Loss = 0.13999, Accuracy = 95.50%: 100%|██████████| 13/13 [00:00<00:00, 80.24it/s]\n",
      "[4 / 20] Train: Loss = 0.08810, Accuracy = 97.32%: 100%|██████████| 572/572 [00:07<00:00, 73.60it/s]\n",
      "[4 / 20]   Val: Loss = 0.12293, Accuracy = 95.99%: 100%|██████████| 13/13 [00:00<00:00, 80.22it/s]\n",
      "[5 / 20] Train: Loss = 0.06092, Accuracy = 98.19%: 100%|██████████| 572/572 [00:07<00:00, 76.69it/s]\n",
      "[5 / 20]   Val: Loss = 0.11390, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 79.76it/s]\n",
      "[6 / 20] Train: Loss = 0.04149, Accuracy = 98.81%: 100%|██████████| 572/572 [00:07<00:00, 75.88it/s]\n",
      "[6 / 20]   Val: Loss = 0.11271, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 82.27it/s]\n",
      "[7 / 20] Train: Loss = 0.02776, Accuracy = 99.24%: 100%|██████████| 572/572 [00:07<00:00, 75.82it/s]\n",
      "[7 / 20]   Val: Loss = 0.11420, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 81.77it/s]\n",
      "[8 / 20] Train: Loss = 0.01824, Accuracy = 99.54%: 100%|██████████| 572/572 [00:07<00:00, 75.20it/s]\n",
      "[8 / 20]   Val: Loss = 0.12299, Accuracy = 96.47%: 100%|██████████| 13/13 [00:00<00:00, 81.54it/s]\n",
      "[9 / 20] Train: Loss = 0.01178, Accuracy = 99.73%: 100%|██████████| 572/572 [00:07<00:00, 75.34it/s]\n",
      "[9 / 20]   Val: Loss = 0.12920, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 79.27it/s]\n",
      "[10 / 20] Train: Loss = 0.00749, Accuracy = 99.84%: 100%|██████████| 572/572 [00:07<00:00, 76.80it/s] \n",
      "[10 / 20]   Val: Loss = 0.13624, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 79.27it/s]\n",
      "[11 / 20] Train: Loss = 0.00452, Accuracy = 99.93%: 100%|██████████| 572/572 [00:07<00:00, 75.90it/s]\n",
      "[11 / 20]   Val: Loss = 0.14475, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 79.76it/s]\n",
      "[12 / 20] Train: Loss = 0.00292, Accuracy = 99.96%: 100%|██████████| 572/572 [00:07<00:00, 74.32it/s]\n",
      "[12 / 20]   Val: Loss = 0.14856, Accuracy = 96.54%: 100%|██████████| 13/13 [00:00<00:00, 78.77it/s]\n",
      "[13 / 20] Train: Loss = 0.00188, Accuracy = 99.98%: 100%|██████████| 572/572 [00:07<00:00, 73.72it/s] \n",
      "[13 / 20]   Val: Loss = 0.15551, Accuracy = 96.54%: 100%|██████████| 13/13 [00:00<00:00, 78.80it/s]\n",
      "[14 / 20] Train: Loss = 0.00138, Accuracy = 99.98%: 100%|██████████| 572/572 [00:07<00:00, 73.67it/s] \n",
      "[14 / 20]   Val: Loss = 0.16162, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 79.19it/s]\n",
      "[15 / 20] Train: Loss = 0.00114, Accuracy = 99.99%: 100%|██████████| 572/572 [00:07<00:00, 72.18it/s]\n",
      "[15 / 20]   Val: Loss = 0.17654, Accuracy = 96.37%: 100%|██████████| 13/13 [00:00<00:00, 74.72it/s]\n",
      "[16 / 20] Train: Loss = 0.00330, Accuracy = 99.91%: 100%|██████████| 572/572 [00:08<00:00, 71.25it/s]\n",
      "[16 / 20]   Val: Loss = 0.17713, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 78.29it/s]\n",
      "[17 / 20] Train: Loss = 0.00317, Accuracy = 99.91%: 100%|██████████| 572/572 [00:08<00:00, 71.22it/s]\n",
      "[17 / 20]   Val: Loss = 0.17312, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 74.64it/s]\n",
      "[18 / 20] Train: Loss = 0.00081, Accuracy = 99.99%: 100%|██████████| 572/572 [00:08<00:00, 71.42it/s]\n",
      "[18 / 20]   Val: Loss = 0.17582, Accuracy = 96.61%: 100%|██████████| 13/13 [00:00<00:00, 76.13it/s]\n",
      "[19 / 20] Train: Loss = 0.00028, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 72.00it/s]\n",
      "[19 / 20]   Val: Loss = 0.17580, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 78.82it/s]\n",
      "[20 / 20] Train: Loss = 0.00021, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 72.25it/s]\n",
      "[20 / 20]   Val: Loss = 0.17950, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 77.11it/s]\n"
     ]
    }
   ],
   "source": [
    "model = BidirectionalLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._embs = nn.Embedding.from_pretrained(torch.FloatTensor(embeddings))\n",
    "        self._lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, lstm_layers_count)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if isinstance(inputs, LongTensor):\n",
    "            embs = self._embs(inputs)\n",
    "        else:\n",
    "            embs = inputs\n",
    "        return self._out_layer(self._lstm(embs)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.74796, Accuracy = 78.38%: 100%|██████████| 572/572 [00:05<00:00, 105.93it/s]\n",
      "[1 / 20]   Val: Loss = 0.36567, Accuracy = 89.17%: 100%|██████████| 13/13 [00:00<00:00, 89.04it/s]\n",
      "[2 / 20] Train: Loss = 0.27925, Accuracy = 91.57%: 100%|██████████| 572/572 [00:05<00:00, 104.09it/s]\n",
      "[2 / 20]   Val: Loss = 0.25161, Accuracy = 92.31%: 100%|██████████| 13/13 [00:00<00:00, 90.82it/s]\n",
      "[3 / 20] Train: Loss = 0.20447, Accuracy = 93.62%: 100%|██████████| 572/572 [00:05<00:00, 104.19it/s]\n",
      "[3 / 20]   Val: Loss = 0.20506, Accuracy = 93.56%: 100%|██████████| 13/13 [00:00<00:00, 90.70it/s]\n",
      "[4 / 20] Train: Loss = 0.16931, Accuracy = 94.63%: 100%|██████████| 572/572 [00:05<00:00, 103.03it/s]\n",
      "[4 / 20]   Val: Loss = 0.18185, Accuracy = 94.19%: 100%|██████████| 13/13 [00:00<00:00, 82.17it/s]\n",
      "[5 / 20] Train: Loss = 0.14928, Accuracy = 95.19%: 100%|██████████| 572/572 [00:05<00:00, 103.78it/s]\n",
      "[5 / 20]   Val: Loss = 0.16815, Accuracy = 94.51%: 100%|██████████| 13/13 [00:00<00:00, 85.53it/s]\n",
      "[6 / 20] Train: Loss = 0.13645, Accuracy = 95.52%: 100%|██████████| 572/572 [00:05<00:00, 105.11it/s]\n",
      "[6 / 20]   Val: Loss = 0.15928, Accuracy = 94.77%: 100%|██████████| 13/13 [00:00<00:00, 90.28it/s]\n",
      "[7 / 20] Train: Loss = 0.12737, Accuracy = 95.75%: 100%|██████████| 572/572 [00:05<00:00, 105.14it/s]\n",
      "[7 / 20]   Val: Loss = 0.15332, Accuracy = 94.90%: 100%|██████████| 13/13 [00:00<00:00, 91.71it/s]\n",
      "[8 / 20] Train: Loss = 0.12043, Accuracy = 95.93%: 100%|██████████| 572/572 [00:05<00:00, 103.08it/s]\n",
      "[8 / 20]   Val: Loss = 0.14792, Accuracy = 95.07%: 100%|██████████| 13/13 [00:00<00:00, 90.91it/s]\n",
      "[9 / 20] Train: Loss = 0.11482, Accuracy = 96.08%: 100%|██████████| 572/572 [00:05<00:00, 103.57it/s]\n",
      "[9 / 20]   Val: Loss = 0.14495, Accuracy = 95.14%: 100%|██████████| 13/13 [00:00<00:00, 86.16it/s]\n",
      "[10 / 20] Train: Loss = 0.11041, Accuracy = 96.21%: 100%|██████████| 572/572 [00:05<00:00, 100.41it/s]\n",
      "[10 / 20]   Val: Loss = 0.14219, Accuracy = 95.21%: 100%|██████████| 13/13 [00:00<00:00, 85.52it/s]\n",
      "[11 / 20] Train: Loss = 0.10679, Accuracy = 96.32%: 100%|██████████| 572/572 [00:05<00:00, 101.84it/s]\n",
      "[11 / 20]   Val: Loss = 0.14148, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 88.43it/s]\n",
      "[12 / 20] Train: Loss = 0.10351, Accuracy = 96.39%: 100%|██████████| 572/572 [00:05<00:00, 100.58it/s]\n",
      "[12 / 20]   Val: Loss = 0.13938, Accuracy = 95.15%: 100%|██████████| 13/13 [00:00<00:00, 87.84it/s]\n",
      "[13 / 20] Train: Loss = 0.10067, Accuracy = 96.49%: 100%|██████████| 572/572 [00:05<00:00, 99.22it/s] \n",
      "[13 / 20]   Val: Loss = 0.13852, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 87.84it/s]\n",
      "[14 / 20] Train: Loss = 0.09812, Accuracy = 96.56%: 100%|██████████| 572/572 [00:05<00:00, 99.60it/s]\n",
      "[14 / 20]   Val: Loss = 0.13664, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 84.12it/s]\n",
      "[15 / 20] Train: Loss = 0.09581, Accuracy = 96.65%: 100%|██████████| 572/572 [00:05<00:00, 97.28it/s]\n",
      "[15 / 20]   Val: Loss = 0.13499, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 88.36it/s]\n",
      "[16 / 20] Train: Loss = 0.09352, Accuracy = 96.73%: 100%|██████████| 572/572 [00:05<00:00, 101.03it/s]\n",
      "[16 / 20]   Val: Loss = 0.13429, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 81.25it/s]\n",
      "[17 / 20] Train: Loss = 0.09178, Accuracy = 96.77%: 100%|██████████| 572/572 [00:05<00:00, 97.02it/s] \n",
      "[17 / 20]   Val: Loss = 0.13720, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 86.09it/s]\n",
      "[18 / 20] Train: Loss = 0.09006, Accuracy = 96.83%: 100%|██████████| 572/572 [00:05<00:00, 102.97it/s]\n",
      "[18 / 20]   Val: Loss = 0.13460, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 80.74it/s]\n",
      "[19 / 20] Train: Loss = 0.08856, Accuracy = 96.86%: 100%|██████████| 572/572 [00:05<00:00, 102.96it/s]\n",
      "[19 / 20]   Val: Loss = 0.13336, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 89.04it/s]\n",
      "[20 / 20] Train: Loss = 0.08709, Accuracy = 96.90%: 100%|██████████| 572/572 [00:05<00:00, 102.03it/s]\n",
      "[20 / 20]   Val: Loss = 0.13411, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 83.32it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9615\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for sentence in test_data:\n",
    "    embs = []\n",
    "    tags = []\n",
    "    for word, tag in sentence:\n",
    "        try:\n",
    "            word_emb = FloatTensor(w2v_model.get_vector(word.lower())).unsqueeze(0)\n",
    "        except KeyError:\n",
    "            word_emb = FloatTensor(np.zeros(w2v_model.vectors.shape[1])).unsqueeze(0)\n",
    "        embs.append(word_emb)\n",
    "        tags.append(tag2ind[tag])\n",
    "    embs = torch.cat(embs, 0).unsqueeze(1)\n",
    "\n",
    "    logits = model(embs).argmax(-1)\n",
    "\n",
    "    correct += (logits.squeeze() == LongTensor(tags)).sum().item()\n",
    "    total += len(tags)\n",
    "\n",
    "print('Test accuracy: {:.4f}'.format(correct / total))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
